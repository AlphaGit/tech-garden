---
title: Law of Large Numbers
tags:
- ai
- machine learning
- courses
- data science
- computer science
- mit ocw
- mit ocw 6.0002
- probability
---

More simulations (a larger sample) usually drives the [[notes/ai/MIT 6.0002/Variance|variance]] down, which is why we have a better [[notes/ai/MIT 6.0002/Confidence intervals|confidence]] in the results. This is the Law of Large numebrs:

> In repeated independent tests with the same actual probability $p$ of a particular outcome in each test, the chance that the franction of times that outcome occurs differs from $p$ converges to zero as the number of trials goes to infinity.
